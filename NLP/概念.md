# 中文切词 
  中文分词这个概念自提出以来，经过多年的发展，主要可以分为三个方法：机械分词方法，统计分词方法，以及两种结合起来的分词。
  
  机械分词方法又叫做基于规则的分词方法：这种分词方法按照一定的规则将待处理的字符串与一个词表词典中的词进行逐一匹配，若在词典中找到某个字符串，则切分，否则不切分。机械分词方法按照匹配规则的方式，又可以分为：正向最大匹配法，逆向最大匹配法和双向匹配法三种。
## 机械分词方法
### 正向最大匹配法  
   正向最大匹配法（Maximum Match Method，MM 法）是指从左向右按最大原则与词典里面的词进行匹配。假设词典中最长词是  mm  个字，那么从待切分文本的最左边取  mm  个字符与词典进行匹配，如果匹配成功，则分词。如果匹配不成功，那么取  m−1m−1  个字符与词典匹配，一直取直到成功匹配为止。
    
    # 判断text的长度是否大于0，如果大于0则进行下面的循环
    while len(text) > 0:
    # 初始化想要取的字符串长度
    # 按照最长词长度初始化
    word_len = max_len_word
    # 对每个字符串可能会有(max_len_word)次循环
    for i in range(0, max_len_word):

        # 令word 等于text的前word_len个字符
        word = text[0:word_len]

        # 为了便于观察过程，我们打印一下当前分割结果
        print('用', word, '进行匹配')

        # 判断word是否在词典dic当中

        # 如果不在词典当中
        if word not in dic:

            #则以word_len - 1
            word_len -= 1

            # 清空word
            word = []

        # 如果word 在词典当中
        else:

            # 更新text串起始位置
            text = text[word_len:]

            # 为了方便观察过程，我们打印一下当前结果
            print('{}   匹配成功，添加进words当中'.format(word))

            # 把匹配成功的word添加进上面创建好的words当中
            words.append(word)

            # 清空word
            word = []
### 逆向最大匹配法
   逆向最大匹配法（ Reverse Maximum Match Method, RMM 法）的原理与正向法基本相同，唯一不同的就是切分的方向与 MM 法相反。逆向法从文本末端开始匹配，每次用末端的最长词长度个字符进行匹配。因为基本原理与 MM 法一样，反向来进行匹配就行。所以这里对算法不再赘述
### 双向最大匹配法
  双向最大匹配法（Bi-direction Matching Method ，BMM）则是将正向匹配法得到的分词结果与逆向匹配法得到的分词结果进行比较，然后按照最大匹配原则，选取次数切分最少的作为结果
## 基于统计规则中文分词
- 语言模型
- 隐马可夫模型
- veterbi 算法
- 中文分词工具

基于统计的分词，一般情况下有两个步骤:
 1. 建立统计语言模型。
 2. 对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。这里就需要用到统计学习算法，如隐马可夫，条件随机场等。
### 语言模型
  语言模型又叫做 N 元文法模型（N-gram）。按照书面解释来说，以长度为 m 的字符串，目的是确定其概率分布  P(w1,w2,⋯,wm)P(w1,w2,⋯,wm) ，其中  w1w1 到  wmwm  依次为文本中的每个词语。
  这个概率可以用链式法则来求：即
  
   $$P\left (w_{1},w_{2}, \cdots ,w_{m}\right )=P\left ( w_{1} \right )*P\left ( w_{2}|w_{1} \right )*P\left ( w_{3}|w_{1},w_{2} \right )\cdots P\left ( w_{m}|w_{1},w_{2},\cdots ,w_{m-1} \right )$$
  
  这里用到的是条件概率的公式  $P\left ( A|B\right )=\frac{P\left ( AB \right )}{P\left ( B \right )}$，即有  $P\left ( AB \right )=P\left ( A|B \right )*P\left ( B \right )$.
  可以看到，当  n=1n=1  时，词与词之间基本没有关系。随着  nn  逐渐增大，每个模型与上文的关系越密切，包含的次序信息也越丰富，但是与此同时计算量也随之大大增加。所以常用的一般为二元模型，三元模型。
### 隐马可夫模型

#### TF-IDF 模型
   这种模型主要是用词汇的统计特征来作为特征集。TF-IDF 由两部分组成：TF（Term frequency，词频），IDF（Inverse document frequency，逆文档频率）两部分组成。
   TF 和 IDF 都很好理解，我们直接来说一下他们的计算公式。
   TF：
       $$tf_{ij} = \frac{n_{ij}}{\sum_{k}n_{kj}}$$